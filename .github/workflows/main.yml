
name: Scrape Unicom Addresses

on:
  schedule:
    - cron: '0 0 * * *' # Run the workflow daily at midnight

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout repository
      uses: actions/checkout@v2

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: 3.8

    - name: Install dependencies
      run: |
        pip install requests
        pip install beautifulsoup4

    - name: Scrape data
      run: |
        import requests
        from bs4 import BeautifulSoup

        # Make a request to the website
        url = "http://www.tcp5.com/list"
        response = requests.get(url)

        # Parse the HTML content of the page
        soup = BeautifulSoup(response.content, "html.parser")

        # Find the data you are interested in
        data = soup.find("table", {"id": "your_table_id"})

        # Extract the data you need
        addresses = []
        for row in data.find_all("tr"):
            cells = row.find_all("td")
            # Do something with the cells
            addresses.append(cells)

        # Write the data to a file
        with open("unicom_addresses.txt", "w") as f:
            for address in addresses:
                f.write(str(address) + "\n")

    - name: Commit and push changes
      uses: actions/github-script@v3
      with:
        script: |
          git config --global user.email "actions@github.com"
          git config --global user.name "GitHub Actions"
          git add unicom_addresses.txt
          git commit -m "Update Unicom addresses"
          git push origin main
